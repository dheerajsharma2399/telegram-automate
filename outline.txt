# Telegram Job Scraper Bot - Implementation Plan

## 🎯 Project Overview

A Telegram bot that monitors job postings in a group, uses LLM for intelligent parsing, and provides control commands for management.

---

## 🏗️ Architecture Design

### Components

```
┌─────────────────────────────────────────────────────────────┐
│                     TELEGRAM BOT (Controller)                │
│  Commands: /start, /stop, /status, /process, /stats         │
└───────────────────┬─────────────────────────────────────────┘
                    │
        ┌───────────┴───────────┐
        ▼                       ▼
┌───────────────┐       ┌──────────────────┐
│  Job Monitor  │       │  Message Queue   │
│   (Listener)  │──────▶│   (SQLite DB)    │
└───────────────┘       └──────────────────┘
                                │
                        ┌───────┴────────┐
                        ▼                ▼
                ┌──────────────┐  ┌─────────────┐
                │ LLM Processor│  │   Status    │
                │  (OpenRouter)│  │   Tracker   │
                └──────┬───────┘  └─────────────┘
                       │
        ┌──────────────┴──────────────┐
        ▼                             ▼
┌──────────────┐              ┌──────────────┐
│ Google Sheets│              │  Local CSV   │
│  (Primary DB)│              │   (Backup)   │
└──────────────┘              └──────────────┘
```

---

## 📊 Database Schema

### Table 1: `raw_messages`
Stores unprocessed messages from the Telegram group.

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER PRIMARY KEY | Auto-increment ID |
| message_id | INTEGER UNIQUE | Telegram message ID |
| message_text | TEXT | Full message content |
| sender_id | INTEGER | Telegram user ID |
| sent_at | TIMESTAMP | When message was sent |
| status | TEXT | 'unprocessed', 'processing', 'processed', 'failed' |
| created_at | TIMESTAMP | When stored in DB |

### Table 2: `processed_jobs`
Stores parsed job postings.

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER PRIMARY KEY | Auto-increment ID |
| raw_message_id | INTEGER | FK to raw_messages |
| job_id | TEXT UNIQUE | Generated job ID |
| first_name | TEXT | Recruiter first name |
| last_name | TEXT | Recruiter last name |
| email | TEXT | Contact email |
| company_name | TEXT | Company name |
| job_role | TEXT | Position/role |
| location | TEXT | Job location |
| eligibility | TEXT | Year/qualifications |
| application_method | TEXT | 'email', 'link', 'phone' |
| status | TEXT | 'pending', 'applied', 'rejected' |
| updated_at | TIMESTAMP | Last status update |
| jd_text | TEXT | Full job description |
| email_subject | TEXT | Email subject line |
| synced_to_sheets | BOOLEAN | Google Sheets sync status |
| created_at | TIMESTAMP | When parsed |

### Table 3: `bot_config`
Stores bot state and configuration.

| Column | Type | Description |
|--------|------|-------------|
| key | TEXT PRIMARY KEY | Config key |
| value | TEXT | Config value |
| updated_at | TIMESTAMP | Last update time |

**Config Keys:**
- `monitoring_status`: 'running', 'stopped'
- `last_processed_message_id`: INTEGER
- `total_messages_processed`: INTEGER
- `total_jobs_extracted`: INTEGER

---

## 🤖 Bot Commands

### `/start`
**Function:** Start the job monitoring service
- Enables message listener
- Updates `monitoring_status` to 'running'
- Responds: "✅ Job monitoring started! Listening for new postings..."

### `/stop`
**Function:** Stop the job monitoring service
- Disables message listener
- Updates `monitoring_status` to 'stopped'
- Responds: "🛑 Job monitoring stopped."

### `/status`
**Function:** Show current system status
**Response Format:**
```
📊 Job Scraper Status

🔄 Monitoring: Running
📨 Unprocessed Messages: 5
✅ Processed Jobs (Today): 12
📧 Jobs with Email: 8
🔗 Jobs with Link/Phone: 4

Last Update: 2025-10-28 14:30:45
```

### `/process`
**Function:** Manually trigger processing of unprocessed messages
- Fetches all messages with status='unprocessed'
- Sends to LLM for parsing
- Updates database and Google Sheets
- Responds with progress updates

### `/stats [days]`
**Function:** Show statistics for last N days (default: 7)
**Response Format:**
```
📈 Statistics (Last 7 Days)

Total Jobs: 45
├─ With Email: 30 (67%)
└─ Without Email: 15 (33%)

Top Companies:
1. Google: 8 jobs
2. Microsoft: 6 jobs
3. Amazon: 5 jobs

Application Status:
✅ Applied: 12
⏳ Pending: 28
❌ Rejected: 5
```

### `/help`
**Function:** Show available commands
- Lists all commands with descriptions

---

## 🧠 LLM Integration (OpenRouter)

### Why Use LLM?
1. **Multi-job Detection:** One message may contain 2-5 job postings
2. **Flexible Parsing:** Job formats vary significantly
3. **Entity Extraction:** Better at extracting company names, roles, emails
4. **Structured Output:** Can return JSON directly

### LLM Workflow

```
Message Text → LLM API (OpenRouter) → Structured JSON → Database
```

### Prompt Template

```python
SYSTEM_PROMPT = """You are an expert job posting parser. Extract ALL job postings from the given text.

For EACH job posting found, extract:
1. company_name: Company or organization name
2. job_role: Position/role title
3. location: Job location(s)
4. eligibility: Year of graduation, degree requirements
5. email: Contact email (if present)
6. phone: Phone number (if present)
7. application_link: External link (if present)
8. recruiter_name: HR/recruiter name (if mentioned)
9. email_subject: Custom subject line (if specified)
10. jd_text: Complete job description

Return a JSON array of job objects. If no jobs found, return empty array.
"""

USER_PROMPT = f"""Parse the following message and extract all job postings:

{message_text}

Return JSON only, no explanation."""
```

### API Configuration
- **Provider:** OpenRouter (openrouter.ai)
- **Model:** `anthropic/claude-3.5-sonnet` (best for structured extraction)
- **Fallback:** `openai/gpt-4o-mini` (cheaper alternative)
- **Max Tokens:** 4000
- **Temperature:** 0.1 (more deterministic)

### Response Format
```json
[
  {
    "company_name": "Google",
    "job_role": "Software Engineer Intern",
    "location": "Bangalore, India",
    "eligibility": "2025/2026 graduates",
    "email": "recruiting@google.com",
    "phone": null,
    "application_link": null,
    "recruiter_name": "Sarah Johnson",
    "email_subject": "Application for SWE Intern - Dheeraj Sharma",
    "jd_text": "Full description here..."
  },
  {
    "company_name": "Microsoft",
    ...
  }
]
```

### Error Handling
- **Retry Logic:** 3 attempts with exponential backoff
- **Fallback:** If LLM fails, use regex-based extraction
- **Logging:** Store failed parses for manual review

---

## 🔄 Processing Flow

### 1. Message Collection (Continuous)
```
Telegram Message → Validate (is job posting?) → Store in raw_messages
                                                  (status: unprocessed)
```

### 2. Processing (Triggered by /process or auto-scheduled)
```
1. Fetch unprocessed messages (LIMIT 10)
2. Update status → 'processing'
3. Send to LLM API
4. Parse JSON response
5. For each job:
   - Generate job_id
   - Extract recruiter name
   - Generate email subject
   - Determine application_method
6. Insert into processed_jobs
7. Sync to Google Sheets
8. Update raw_message status → 'processed'
9. Update bot_config counters
```

### 3. Auto-Processing (Background Task)
- Run every 5 minutes
- Process up to 10 unprocessed messages per batch
- Prevents rate limiting on OpenRouter

---

## 🚀 Deployment on Render

### Service Configuration

**Type:** Web Service (for bot) + Background Worker (for monitoring)

### Environment Variables
```bash
# Telegram
TELEGRAM_API_ID=your_api_id
TELEGRAM_API_HASH=your_api_hash
TELEGRAM_PHONE=your_phone
TELEGRAM_BOT_TOKEN=your_bot_token
TELEGRAM_GROUP_USERNAME=job_group

# OpenRouter
OPENROUTER_API_KEY=your_openrouter_key
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# Google Sheets
GOOGLE_CREDENTIALS_JSON={"type":"service_account",...}
SPREADSHEET_NAME=Job Postings Database

# Database
DATABASE_URL=sqlite:///jobs.db
```

### File Structure
```
job-scraper-bot/
├── main.py                 # Bot command handler
├── monitor.py              # Message listener
├── processor.py            # LLM processing logic
├── database.py             # Database operations
├── sheets_sync.py          # Google Sheets sync
├── config.py               # Configuration
├── requirements.txt        # Dependencies
├── Dockerfile              # (Optional) Container config
├── render.yaml             # Render deployment config
└── credentials.json        # Google service account (gitignored)
```

### requirements.txt
```
telethon>=1.35.0
python-telegram-bot>=20.7
gspread>=5.12.0
google-auth>=2.25.0
openai>=1.3.0  # For OpenRouter API
aiohttp>=3.9.0
python-dotenv>=1.0.0
APScheduler>=3.10.0
```

### Render Configuration (render.yaml)
```yaml
services:
  - type: web
    name: job-scraper-bot
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: python main.py
    envVars:
      - key: TELEGRAM_BOT_TOKEN
        sync: false
      - key: OPENROUTER_API_KEY
        sync: false
```

### Keep-Alive Strategy
- Use APScheduler for background tasks
- Health check endpoint: `/health`
- Render free tier: 15 min sleep after inactivity
- **Solution:** Use cron-job.org to ping `/health` every 10 minutes

---

## ⚡ Performance Optimizations

### 1. Rate Limiting
- **OpenRouter:** Max 60 requests/min (Claude 3.5 Sonnet)
- **Telegram API:** Max 30 messages/second
- **Google Sheets:** Max 60 requests/min per user

**Strategy:** Batch processing with delays

### 2. Caching
- Cache LLM responses for duplicate messages
- Store message hashes to detect duplicates

### 3. Queue Management
- Process messages in FIFO order
- Priority queue for manual `/process` commands

### 4. Cost Management
- **OpenRouter Costs:**
  - Claude 3.5 Sonnet: ~$3 per 1M input tokens
  - GPT-4o Mini: ~$0.15 per 1M input tokens
- **Estimation:** ~500 jobs/month = ~$2-5/month

---

## 🔒 Security Considerations

1. **Environment Variables:** Never commit credentials
2. **Bot Access Control:** Whitelist authorized user IDs
3. **Database Encryption:** SQLite encryption for sensitive data
4. **API Key Rotation:** Monthly rotation policy
5. **Google Sheets Permissions:** Service account with minimal access

---

## 📋 Implementation Phases

### Phase 1: Core Setup (Days 1-2)
- [ ] Set up SQLite database with schema
- [ ] Implement basic bot commands (/start, /stop, /status)
- [ ] Create message listener
- [ ] Test message collection

### Phase 2: LLM Integration (Days 3-4)
- [ ] Set up OpenRouter API client
- [ ] Create prompt templates
- [ ] Implement job parsing logic
- [ ] Test with sample messages

### Phase 3: Processing Pipeline (Days 5-6)
- [ ] Build processing queue
- [ ] Implement background worker
- [ ] Add error handling & retries
- [ ] Create Google Sheets sync

### Phase 4: Advanced Features (Days 7-8)
- [ ] Add /stats command
- [ ] Implement auto-processing scheduler
- [ ] Add duplicate detection
- [ ] Build dashboard (optional)

### Phase 5: Deployment (Day 9)
- [ ] Set up Render account
- [ ] Configure environment variables
- [ ] Deploy and test
- [ ] Set up monitoring

### Phase 6: Testing & Refinement (Day 10)
- [ ] Load testing
- [ ] Prompt optimization
- [ ] Bug fixes
- [ ] Documentation

---

## 🧪 Testing Strategy

### Unit Tests
- Database operations
- LLM parsing logic
- Google Sheets sync

### Integration Tests
- Bot command handling
- End-to-end message flow
- API error scenarios

### Manual Testing
- Send various job posting formats
- Test with multiple jobs in one message
- Verify Google Sheets updates

---

## 📊 Monitoring & Logging

### Logs to Track
1. Messages received count
2. LLM API calls & costs
3. Processing errors
4. Google Sheets sync status
5. Bot command usage

### Alerting
- Telegram message to admin on critical errors
- Daily summary report

---

## 🎯 Success Metrics

1. **Accuracy:** >95% correct job extraction
2. **Uptime:** >99% availability
3. **Latency:** <30 seconds from message to database
4. **Cost:** <$10/month operational cost

---

## 🚧 Future Enhancements

1. **Web Dashboard:** React frontend for job management
2. **Auto-Apply:** Integration with email to auto-send applications
3. **ML Ranking:** Score jobs based on your profile
4. **Calendar Integration:** Track application deadlines
5. **Resume Customization:** LLM-powered resume tailoring per job
6. **Status Tracking:** Update application status automatically

---

## 💡 Key Decisions Made

| Decision | Option Chosen | Reason |
|----------|---------------|--------|
| Database | SQLite | Simple, portable, no server needed |
| LLM Provider | OpenRouter | Access to multiple models, good pricing |
| LLM Model | Claude 3.5 Sonnet | Best structured output, JSON mode |
| Hosting | Render | Free tier, easy deployment, Python support |
| Bot Framework | python-telegram-bot | Mature, well-documented |
| Processing | Async queue | Prevents blocking, better reliability |

---

## ✅ Ready to Implement?

This plan provides a complete blueprint. Would you like me to start implementing the code based on this plan? I can create:

1. **Database setup script** (database.py)
2. **Main bot handler** (main.py)
3. **LLM processor** (processor.py)
4. **Message monitor** (monitor.py)

Let me know which component to build first! 🚀